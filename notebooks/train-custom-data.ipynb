{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173befbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import plot_images, sliding_average\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "# from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def _validate_root_dir(root):\n",
    "    # todo: raise exception or warning\n",
    "    pass\n",
    "\n",
    "def _validate_train_flag(train: bool, valid: bool, test: bool):\n",
    "    assert [train, valid, test].count(True)==1, \"one of train, valid & test must be true.\"        \n",
    "\n",
    "class CustomDataset(Dataset):        \n",
    "    def __init__(self, root,\n",
    "                 train: bool = False, valid: bool = False, test: bool = False,\n",
    "                 transform=None, target_transform=None,):\n",
    "\n",
    "        _validate_root_dir(root)\n",
    "        _validate_train_flag(train, valid, test)        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if train:\n",
    "            self.data_dir = Path(root)/'train'\n",
    "        elif valid:\n",
    "            self.data_dir = Path(root)/'valid'\n",
    "        elif test:\n",
    "            self.data_dir = Path(root)/'test'        \n",
    "    \n",
    "        self._image_paths = sorted(\n",
    "            list(self.data_dir.glob(\"**/*.jpg\"))+\n",
    "            list(self.data_dir.glob(\"**/*.jpeg\"))+\n",
    "            list(self.data_dir.glob(\"**/*.png\")))\n",
    "        self._image_labels = [int(i.parent.name) for i in self._image_paths]\n",
    "        assert len(self._image_paths)==len(self._image_labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = Image.open(str(self._image_paths[idx]))\n",
    "        y = self._image_labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(x)\n",
    "        return x, y\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self._image_labels\n",
    "\n",
    "image_size = 128\n",
    "data_dir = '/home/crx/150gdata/数据集存放/few-shot-test/GTSRB_Challenge/data'#'../data/ssddir-traffic-signs'\n",
    "\n",
    "# train set is list of (x, y) \n",
    "# where x is single image and y is label corresponding to it.\n",
    "train_set = CustomDataset(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            # Omniglot images have 1 channel, but our model will expect 3-channel images\n",
    "            # transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize([int(image_size * 1.5), int(image_size * 1.5)]),\n",
    "            transforms.RandomPerspective(0.5, 0.8),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2, contrast=0.15, saturation=0, hue=0,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "valid_set = CustomDataset(\n",
    "    root=data_dir,\n",
    "    valid=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            # If images have 1 channel, our model will expect 3-channel images\n",
    "            # transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b134e7b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of labels in the dataset (0 must be greater or equal to n_way (5).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_289600/74639256.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m valid_sampler = TaskSampler(\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mvalid_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_way\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mN_WAY\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_shot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mN_SHOT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_query\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mN_QUERY\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_tasks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mN_EVALUATION_TASKS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/桌面/2023FR/easy-few-shot-learning-master/easyfsl/samplers/task_sampler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, dataset, n_way, n_shot, n_query, n_tasks)\u001B[0m\n\u001B[1;32m     50\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems_per_label\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_dataset_size_fits_sampler_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/桌面/2023FR/easy-few-shot-learning-master/easyfsl/samplers/task_sampler.py\u001B[0m in \u001B[0;36m_check_dataset_size_fits_sampler_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0mCheck\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0msize\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mcompatible\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msampler\u001B[0m \u001B[0mparameters\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \"\"\"\n\u001B[0;32m--> 172\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_dataset_has_enough_labels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_dataset_has_enough_items_per_label\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/桌面/2023FR/easy-few-shot-learning-master/easyfsl/samplers/task_sampler.py\u001B[0m in \u001B[0;36m_check_dataset_has_enough_labels\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_way\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems_per_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m             raise ValueError(\n\u001B[0;32m--> 178\u001B[0;31m                 \u001B[0;34mf\"The number of labels in the dataset ({len(self.items_per_label)} \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    179\u001B[0m                 \u001B[0;34mf\"must be greater or equal to n_way ({self.n_way}).\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m             )\n",
      "\u001B[0;31mValueError\u001B[0m: The number of labels in the dataset (0 must be greater or equal to n_way (5)."
     ]
    }
   ],
   "source": [
    "N_WAY = 5 # 5  # Number of classes in a task\n",
    "N_SHOT = 3 # 5  # Number of images per class in the support set\n",
    "N_QUERY = 5 # 10  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100 \n",
    "\n",
    "valid_sampler = TaskSampler(\n",
    "    valid_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_sampler=valid_sampler,\n",
    "    # num_workers=12,\n",
    "    pin_memory=True,\n",
    "    collate_fn=valid_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03e69db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_289600/1723797796.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mexample_query_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mexample_class_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m ) = next(iter(valid_loader))\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mplot_images\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexample_support_images\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"support images\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimages_per_row\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mN_SHOT\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(valid_loader))\n",
    "\n",
    "plot_images(example_support_images, \"support images\", images_per_row=N_SHOT)\n",
    "plot_images(example_query_images, \"query images\", images_per_row=N_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb193d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetworks(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super(PrototypicalNetworks, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        support_images: torch.Tensor,\n",
    "        support_labels: torch.Tensor,\n",
    "        query_images: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict query labels using labeled support images.\n",
    "        \"\"\"\n",
    "        # Extract the features of support and query images\n",
    "        z_support = self.backbone.forward(support_images)\n",
    "        z_query = self.backbone.forward(query_images)\n",
    "\n",
    "        # Infer the number of different classes from the labels of the support set\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "        z_proto = torch.cat(\n",
    "            [\n",
    "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
    "                for label in range(n_way)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        dists = torch.cdist(z_query, z_proto)\n",
    "\n",
    "        # And here is the super complicated operation to transform those distances into classification scores!\n",
    "        scores = -dists\n",
    "        return scores\n",
    "\n",
    "\n",
    "convolutional_network = resnet18(pretrained=True)\n",
    "convolutional_network.fc = nn.Flatten()\n",
    "print(convolutional_network)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = PrototypicalNetworks(convolutional_network).cuda()\n",
    "model = PrototypicalNetworks(convolutional_network).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example_scores = model(\n",
    "    example_support_images.to(device), #.cuda(),\n",
    "    example_support_labels.to(device),#.cuda(),\n",
    "    example_query_images.to(device), #.cuda(),\n",
    ").detach()\n",
    "\n",
    "_, example_predicted_labels = torch.max(example_scores.data, 1)\n",
    "\n",
    "print(\"Ground Truth / Predicted\")\n",
    "for i in range(len(example_query_labels)):\n",
    "    print(\n",
    "        f\"{valid_set._image_labels[example_class_ids[example_query_labels[i]]]} / {valid_set._image_labels[example_class_ids[example_predicted_labels[i]]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_one_task(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    ") -> [int, int]:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.max(\n",
    "#             model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n",
    "            model(support_images.to(device), support_labels.to(device), query_images.to(device))\n",
    "            .detach()\n",
    "            .data,\n",
    "            1,\n",
    "        )[1]\n",
    "        == query_labels.to(device)#.cuda()\n",
    "    ).sum().item(), len(query_labels)\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader):\n",
    "    # We'll count everything and compute the ratio at the end\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
    "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for episode_index, (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            class_ids,\n",
    "        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "\n",
    "            correct, total = evaluate_on_one_task(\n",
    "                support_images, support_labels, query_images, query_labels\n",
    "            )\n",
    "\n",
    "            total_predictions += total\n",
    "            correct_predictions += correct\n",
    "\n",
    "    print(\n",
    "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "evaluate(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING_EPISODES = 40000\n",
    "N_VALIDATION_TASKS = 100\n",
    "\n",
    "train_sampler = TaskSampler(\n",
    "    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_sampler=train_sampler,\n",
    "    # num_workers=12,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def fit(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    ") -> float:\n",
    "    optimizer.zero_grad()\n",
    "    classification_scores = model(\n",
    "#         support_images.cuda(), support_labels.cuda(), query_images.cuda()\n",
    "        support_images.to(device), support_labels.to(device), query_images.to(device)\n",
    "    )\n",
    "\n",
    "    loss = criterion(classification_scores, query_labels.to(device))#query_labels.cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121550a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model yourself with this cell\n",
    "\n",
    "log_update_frequency = 10\n",
    "\n",
    "all_loss = []\n",
    "model.train()\n",
    "with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n",
    "    for episode_index, (\n",
    "        support_images,\n",
    "        support_labels,\n",
    "        query_images,\n",
    "        query_labels,\n",
    "        _,\n",
    "    ) in tqdm_train:\n",
    "        loss_value = fit(support_images, support_labels, query_images, query_labels)\n",
    "        all_loss.append(loss_value)\n",
    "\n",
    "        if episode_index % log_update_frequency == 0:\n",
    "            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c924b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HOLDOUT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = CustomDataset(\n",
    "    root=data_dir,\n",
    "    test=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            # If images have 1 channel, our model will expect 3-channel images\n",
    "            # transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "N_WAY = 5 # 5  # Number of classes in a task\n",
    "N_SHOT = 3 # 5  # Number of images per class in the support set\n",
    "N_QUERY = 5 # 10  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100 \n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    # num_workers=12,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on query example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}